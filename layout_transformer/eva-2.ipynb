{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model from /home/weiran/Projects/RvNN-Layout/LayoutTrans/layout_transformer/logs/magazine_1K/checkpoints/checkpoint.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import math\n",
    "import logging\n",
    "from dataset import MNISTLayout, JSONLayout\n",
    "from model import GPT, GPTConfig\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from utils import top_k_logits, sample\n",
    "\n",
    "name = \"magazine_1K\"\n",
    "path = '/home/weiran/Projects/RvNN-Layout/LayoutTrans/layout_transformer/logs/' + name\n",
    "args = torch.load(path + '/conf.pth')\n",
    "args.load_model = path + '/checkpoints/checkpoint.pth'\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "log_dir = os.path.join(args.log_dir, args.exp)\n",
    "eval_dir = os.path.join(log_dir, \"eval\")\n",
    "os.makedirs(eval_dir, exist_ok=True)\n",
    "\n",
    "train_dataset = JSONLayout(args.train_json)\n",
    "\n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.max_length,n_layer=args.n_layer, n_head=args.n_head, n_embd=args.n_embd)\n",
    "\n",
    "model = GPT(mconf).to(device)\n",
    "\n",
    "print(f\"loading model from {args.load_model}\")\n",
    "model.load_state_dict(torch.load(args.load_model, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import trim_tokens\n",
    "from PIL import Image, ImageDraw, ImageOps\n",
    "\n",
    "\n",
    "def render(self, layout):\n",
    "    img = Image.new('RGB', (256, 256), color=(255, 255, 255))\n",
    "    draw = ImageDraw.Draw(img, 'RGBA')\n",
    "    layout = layout.reshape(-1)\n",
    "    layout = trim_tokens(layout, self.bos_token, self.eos_token, self.pad_token)\n",
    "    layout = layout[: len(layout) // 5 * 5].reshape(-1, 5)\n",
    "    box = layout[:, 1:].astype(np.float32)\n",
    "    box[:, [0, 1]] = box[:, [0, 1]] / (self.size - 1) * 255\n",
    "    box[:, [2, 3]] = box[:, [2, 3]] / self.size * 256\n",
    "    box[:, [2, 3]] = box[:, [0, 1]] + box[:, [2, 3]]\n",
    "    \n",
    "    layoutText = []\n",
    "\n",
    "    for i in range(len(layout)):\n",
    "        x1, y1, x2, y2 = box[i]\n",
    "        cat = layout[i][0]\n",
    "        layoutText.append([cat - 255, x1 / 256, y1 / 256, x2 / 256, y2 / 256])\n",
    "        col = self.colors[cat-self.size] if 0 <= cat-self.size < len(self.colors) else [0, 0, 0]\n",
    "        draw.rectangle([x1, y1, x2, y2],\n",
    "                        outline=tuple(col) + (200,),\n",
    "                        fill=tuple(col) + (64,),\n",
    "                        width=2)\n",
    "\n",
    "    # Add border around image\n",
    "    img = ImageOps.expand(img, border=2)\n",
    "    return img, layoutText\n",
    "\n",
    "\n",
    "def saveTxt(list, path):\n",
    "    with open(path, 'w') as f:\n",
    "        for item in list:\n",
    "            f.write(\" \".join(str(x) for x in item) + \"\\n\")\n",
    "\n",
    "\n",
    "def sample(model, x, steps, temperature=1.0, sample=False, top_k=None):\n",
    "    \"\"\"\n",
    "    take a conditioning sequence of indices in x (of shape (b,t)) and predict the next token in\n",
    "    the sequence, feeding the predictions back into the model each time. Clearly the sampling\n",
    "    has quadratic complexity unlike an RNN that is only linear, and has a finite context window\n",
    "    of block_size, unlike an RNN that has an infinite context window.\n",
    "    \"\"\"\n",
    "    block_size = model.module.get_block_size() if hasattr(model, \"module\") else model.get_block_size()\n",
    "    model.eval()\n",
    "    for k in range(steps):\n",
    "        x_cond = x if x.size(1) <= block_size else x[:, -block_size:]  # crop context if needed\n",
    "        logits, _ = model(x_cond)\n",
    "        # pluck the logits at the final step and scale by temperature\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        # optionally crop probabilities to only the top k options\n",
    "        if top_k is not None:\n",
    "            logits = top_k_logits(logits, top_k)\n",
    "        # apply softmax to convert to probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # sample from the distribution or take the most likely\n",
    "        if sample:\n",
    "            ix = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            _, ix = torch.topk(probs, k=1, dim=-1)\n",
    "        # append to the sequence and continue\n",
    "        x = torch.cat((x, ix), dim=1)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layout reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layout_reconstruction(start, stop):\n",
    "    recon_dir = os.path.join(eval_dir, 'recon')\n",
    "    os.makedirs(recon_dir, exist_ok=True)\n",
    "    \n",
    "    for i in range(start, stop):\n",
    "        x = train_dataset[i][0]\n",
    "\n",
    "        x_cond = x.to(device).unsqueeze(0)\n",
    "        logits, _ = model(x_cond)\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        _, y = torch.topk(probs, k=1, dim=-1)\n",
    "\n",
    "        '''\n",
    "        y[:, :, 0].size() : [1, 297]\n",
    "        conat x_cond[:, :1] and y[:, :, 0] to add a bos token\n",
    "        '''\n",
    "\n",
    "        layout = torch.cat((x_cond[:, :1], y[:, :, 0]), dim=1).detach().cpu().numpy()\n",
    "        layout, txt = render(train_dataset, layout)\n",
    "        layout.save(os.path.join(recon_dir, f'{i}_PRED.png'))\n",
    "        saveTxt(txt, os.path.join(recon_dir, f'{i}_PRED.txt'))\n",
    "        \n",
    "        layout = x_cond.detach().cpu().numpy()\n",
    "        layout, txt = render(train_dataset, layout)\n",
    "        layout.save(os.path.join(recon_dir, f'{i}_GT.png'))\n",
    "        saveTxt(txt, os.path.join(recon_dir, f'{i}_GT.txt'))\n",
    "\n",
    "layout_reconstruction(0, 300)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layout generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 300/300 [03:09<00:00,  1.58it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random as rand\n",
    "from tqdm import tqdm\n",
    "\n",
    "valid_dataset = JSONLayout(args.val_json, max_length=train_dataset.max_length)\n",
    "\n",
    "def condition_generation(num, random=False, top_k=None):\n",
    "    gen_dir = os.path.join(eval_dir, 'generation-test')\n",
    "    os.makedirs(gen_dir, exist_ok=True)\n",
    "\n",
    "    # 使用 tqdm 创建一个进度条\n",
    "    for i in tqdm(range(num), desc='Generating'):\n",
    "        number = rand.randint(0, len(valid_dataset) - 1)\n",
    "        x = valid_dataset[number][0][:6].unsqueeze(0).to(device)\n",
    "        layout = sample(model, x, steps=train_dataset.max_length,temperature=1.0, sample=random, top_k=top_k).detach().cpu().numpy()\n",
    "        layout, txt = render(train_dataset, layout)\n",
    "        layout.save(os.path.join(gen_dir, f'random_gen_{i}.png'))\n",
    "        saveTxt(txt, os.path.join(gen_dir, f'random_gen_{i}.txt'))\n",
    "\n",
    "condition_generation(300, random=True, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 300/300 [03:56<00:00,  1.27it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def sample_generation(num, random=False, top_k=None):\n",
    "    gen_dir = os.path.join(eval_dir, 'generation')\n",
    "    os.makedirs(gen_dir, exist_ok=True)\n",
    "    \n",
    "    for i in tqdm(range(num), desc='Generating'):\n",
    "        x = torch.tensor([[261]]).to(device)\n",
    "        layout = sample(model, x, steps=train_dataset.max_length,temperature=1.0, sample=random, top_k=top_k).detach().cpu().numpy()\n",
    "        layout, txt = render(train_dataset, layout)\n",
    "        layout.save(os.path.join(gen_dir, f'random_gen_{i}.png'))\n",
    "        saveTxt(txt, os.path.join(gen_dir, f'random_gen_{i}.txt'))\n",
    "\n",
    "sample_generation(300, random=True, top_k=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_logits(logits, k):\n",
    "    v, ix = torch.topk(logits, k)\n",
    "    out = logits.clone()\n",
    "    out[out < v[...,[-1]]] = -float('Inf')\n",
    "    return out\n",
    "\n",
    "def sample_dec(model, x, x_tag, steps, temperature=1.0, sample=False, top_k=None):\n",
    "    block_size = model.module.get_block_size() if hasattr(model, \"module\") else model.get_block_size()\n",
    "    x_cond = x if x.size(1) <= block_size else x[:, -block_size:]  # crop context if needed\n",
    "    model.eval()\n",
    "    logits = model.decoder(x_cond)\n",
    "    logits = logits[:,:5, :] / temperature\n",
    "    \n",
    "    for i in range(logits.shape[1]):\n",
    "        l = logits[:, i, :]\n",
    "        \n",
    "        if top_k is not None:\n",
    "            l = top_k_logits(l, top_k)\n",
    "        \n",
    "        probs = F.softmax(l, dim=-1)\n",
    "        \n",
    "        if sample:\n",
    "            ix = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            _, ix = torch.topk(probs, k=1, dim=-1)\n",
    "        \n",
    "        print(ix)\n",
    "\n",
    "    return ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cond = train_dataset[0][0].unsqueeze(dim=0).to(device)\n",
    "\n",
    "x = model.encoder(x_cond[:, :6])\n",
    "# x[0][0]\n",
    "\n",
    "random_tensor = torch.randn_like(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[261, 260,  50,  17, 150, 106, 256,  17, 125, 217,  11, 256,  17, 140,\n",
       "         104,  20, 256, 129, 140, 104,   7, 256, 129, 147, 104,  23, 256,  17,\n",
       "         160, 104,  10, 257,  17, 177,   9,   3, 256,  17, 179, 217,   5, 259,\n",
       "          17, 186, 217,  50, 262, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[261]], device='cuda:0')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cond[:, :1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[261, 258,   3,   3,  36,  20, 258,  92,   3,  36,  15, 258, 131,\n",
       "          3,  36,  15, 258,  45,  45,  36,  75, 258, 131,   3,  36,  75,\n",
       "        258,  45,  75,  36,  88, 258,  45,  88,  32,  11, 258, 131,  75,\n",
       "         36,  88, 258, 131,  75,  36,  32, 257, 131, 173,  36,  32, 258,\n",
       "          3, 173,  75,  32, 258, 131, 173,  36,  32, 258, 131, 173,  36,\n",
       "         20, 262, 143, 173,  41,  15, 262,  32, 203,  36,  11, 262,  75,\n",
       "        203,  41,  32, 262,  11, 203,  36,  32, 257,   3, 211,  11, 262,\n",
       "        211,  11]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[261]]).to(device)\n",
    "layout = sample(model, x, steps=train_dataset.max_length,temperature=1.0, sample=True, top_k=5).detach().cpu().numpy()\n",
    "layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 0.01171875, 0.01171875, 0.15234375, 0.08984375],\n",
       " [3, 0.359375, 0.01171875, 0.5, 0.0703125],\n",
       " [3, 0.51171875, 0.01171875, 0.65234375, 0.0703125],\n",
       " [3, 0.17578125, 0.17578125, 0.31640625, 0.46875],\n",
       " [3, 0.51171875, 0.01171875, 0.65234375, 0.3046875],\n",
       " [3, 0.17578125, 0.29296875, 0.31640625, 0.63671875],\n",
       " [3, 0.17578125, 0.34375, 0.30078125, 0.38671875],\n",
       " [3, 0.51171875, 0.29296875, 0.65234375, 0.63671875],\n",
       " [3, 0.51171875, 0.29296875, 0.65234375, 0.41796875],\n",
       " [2, 0.51171875, 0.67578125, 0.65234375, 0.80078125],\n",
       " [3, 0.01171875, 0.67578125, 0.3046875, 0.80078125],\n",
       " [3, 0.51171875, 0.67578125, 0.65234375, 0.80078125],\n",
       " [3, 0.51171875, 0.67578125, 0.65234375, 0.75390625]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, text = render(train_dataset, layout)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "saveTxt(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layouts = self.fixed_x.detach().cpu().numpy()\n",
    "input_layouts = [self.train_dataset.render(layout) for layout in layouts]\n",
    "for i, layout in enumerate(input_layouts):\n",
    "    layout = self.train_dataset.render(layout)\n",
    "    layout.save(os.path.join(self.config.samples_dir, f'input_{epoch:02d}_{i:02d}.png'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
