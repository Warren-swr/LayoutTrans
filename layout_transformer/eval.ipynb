{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model from /home/weiran/Projects/RvNN-Layout/LayoutTrans/layout_transformer/logs/magazine_0.3K/checkpoints/checkpoint.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import math\n",
    "import logging\n",
    "from dataset import MNISTLayout, JSONLayout\n",
    "from model import GPT, GPTConfig\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from utils import top_k_logits, sample\n",
    "\n",
    "path = '/home/weiran/Projects/RvNN-Layout/LayoutTrans/layout_transformer/logs/magazine_0.3K'\n",
    "args = torch.load(path + '/conf.pth')\n",
    "args.load_model = path + '/checkpoints/checkpoint.pth'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "log_dir = os.path.join(args.log_dir, args.exp)\n",
    "eval_dir = os.path.join(log_dir, \"eval\")\n",
    "os.makedirs(eval_dir, exist_ok=True)\n",
    "\n",
    "train_dataset = JSONLayout(args.train_json)\n",
    "\n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.max_length,n_layer=args.n_layer, n_head=args.n_head, n_embd=args.n_embd)\n",
    "\n",
    "model = GPT(mconf).to(device)\n",
    "\n",
    "print(f\"loading model from {args.load_model}\")\n",
    "model.load_state_dict(torch.load(args.load_model, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layout_reconstruction(start, stop):\n",
    "    for i in range(start, stop):\n",
    "        x = train_dataset[i][0]\n",
    "\n",
    "        x_cond = x.to(device).unsqueeze(0)\n",
    "        logits, _, _ = model(x_cond)\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        _, y = torch.topk(probs, k=1, dim=-1)\n",
    "\n",
    "        '''\n",
    "        y[:, :, 0].size() : [1, 297]\n",
    "        conat x_cond[:, :1] and y[:, :, 0] to add a bos token\n",
    "        '''\n",
    "\n",
    "        layout = torch.cat((x_cond[:, :1], y[:, :, 0]), dim=1).detach().cpu().numpy()\n",
    "        layout = train_dataset.render(layout)\n",
    "        layout.save(os.path.join(eval_dir, f'recon_{i:02d}.png'))\n",
    "        \n",
    "        layout = x_cond.detach().cpu().numpy()\n",
    "        layout = train_dataset.render(layout)\n",
    "        layout.save(os.path.join(eval_dir, f'input_{i:02d}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = JSONLayout(args.train_json)\n",
    "layout_reconstruction(0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_logits(logits, k):\n",
    "    v, ix = torch.topk(logits, k)\n",
    "    out = logits.clone()\n",
    "    out[out < v[...,[-1]]] = -float('Inf')\n",
    "    return out\n",
    "\n",
    "def sample_dec(model, x, x_tag, steps, temperature=1.0, sample=False, top_k=None):\n",
    "    block_size = model.module.get_block_size() if hasattr(model, \"module\") else model.get_block_size()\n",
    "    x_cond = x if x.size(1) <= block_size else x[:, -block_size:]  # crop context if needed\n",
    "    model.eval()\n",
    "    logits = model.decoder(x_cond)\n",
    "    logits = logits[:,:5, :] / temperature\n",
    "    \n",
    "    for i in range(logits.shape[1]):\n",
    "        l = logits[:, i, :]\n",
    "        \n",
    "        if top_k is not None:\n",
    "            l = top_k_logits(l, top_k)\n",
    "        \n",
    "        probs = F.softmax(l, dim=-1)\n",
    "        \n",
    "        if sample:\n",
    "            ix = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            _, ix = torch.topk(probs, k=1, dim=-1)\n",
    "        \n",
    "        print(ix)\n",
    "\n",
    "    return ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cond = train_dataset[0][0].unsqueeze(dim=0).to(device)\n",
    "\n",
    "x = model.encoder(x_cond[:, :6])\n",
    "# x[0][0]\n",
    "\n",
    "random_tensor = torch.randn_like(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[261, 260,  50,  17, 150, 106, 256,  17, 125, 217,  11, 256,  17, 140,\n",
       "         104,  20, 256, 129, 140, 104,   7, 256, 129, 147, 104,  23, 256,  17,\n",
       "         160, 104,  10, 257,  17, 177,   9,   3, 256,  17, 179, 217,   5, 259,\n",
       "          17, 186, 217,  50, 262, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
       "         263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[261]], device='cuda:0')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cond[:, :1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, x, steps, temperature=1.0, sample=False, top_k=None):\n",
    "    \"\"\"\n",
    "    take a conditioning sequence of indices in x (of shape (b,t)) and predict the next token in\n",
    "    the sequence, feeding the predictions back into the model each time. Clearly the sampling\n",
    "    has quadratic complexity unlike an RNN that is only linear, and has a finite context window\n",
    "    of block_size, unlike an RNN that has an infinite context window.\n",
    "    \"\"\"\n",
    "    block_size = model.module.get_block_size() if hasattr(model, \"module\") else model.get_block_size()\n",
    "    model.eval()\n",
    "    for k in range(steps):\n",
    "        x_cond = x if x.size(1) <= block_size else x[:, -block_size:]  # crop context if needed\n",
    "        logits, _ = model(x_cond)\n",
    "        # pluck the logits at the final step and scale by temperature\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        # optionally crop probabilities to only the top k options\n",
    "        if top_k is not None:\n",
    "            logits = top_k_logits(logits, top_k)\n",
    "        # apply softmax to convert to probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # sample from the distribution or take the most likely\n",
    "        if sample:\n",
    "            ix = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            _, ix = torch.topk(probs, k=1, dim=-1)\n",
    "        # append to the sequence and continue\n",
    "        x = torch.cat((x, ix), dim=1)\n",
    "\n",
    "    return x\n",
    "\n",
    "def sample_generation(num, random=False, top_k=None):\n",
    "    for i in range(num):\n",
    "        x = torch.tensor([[261]]).to(device)\n",
    "        layout = sample(model, x, steps=train_dataset.max_length,temperature=1.0, sample=random, top_k=top_k).detach().cpu().numpy()\n",
    "        layout = train_dataset.render(layout)\n",
    "        layout.save(os.path.join(eval_dir, f'rendom_gen_{i}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_generation(50, random=True, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[261, 257,   3,   3, 190, 254, 258,  20,  20,  41,  41, 258,  67,\n",
       "         20,  36,  15, 258,  20,  67,  41,  24, 258,  20,  67,  41,  54,\n",
       "        258,  20, 118,  41,  66, 262,  49,  67,  36,  24, 262,  37, 118,\n",
       "         41,  28, 258,  20, 160,  41,  28, 262, 113, 186,  41,  58, 258,\n",
       "         20, 211,  41,  28, 262,  79, 224,  41,  28, 262,  37, 224,  41,\n",
       "         24, 262,  24, 224,  19,  24, 262, 203, 224,  32,  24, 262,  62,\n",
       "        224,  32,  15, 262,  24, 224,  41, 224, 262,  32, 224,  32,  24,\n",
       "        262, 224, 224, 100,  37, 262, 224]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[261]]).to(device)\n",
    "layout = sample(model, x, steps=train_dataset.max_length,temperature=1.0, sample=False, top_k=None).detach().cpu().numpy()\n",
    "layout"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
